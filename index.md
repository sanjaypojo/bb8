> Humans can rapidly perceive their environment to make judgements about the goals and actions of others around them. In fact, developmental studies show that infants as young as six months are able to infer goals or desires based on people’s “reaching” motion [[10]](#ref1). However, building computational models for these sophisticated cognitive abilities continues to be a challenge. As robots move from the assembly line to becoming collaborators with people in a range of domains, it is necessary to consider models for robot behavior that are intuitive from the perspective of a human observer or collaborator [[5]](#ref5). In this review, we aim to explore research in brain and cognitive science, human-robot interaction, and artificial intelligence in order to understand how human beings predict the motion goals of others, with a specific focus on we can apply these learnings to better models for action parsing and goal inference in robot behavior for human-robot collaboration.

## Introduction
The advent of innovations in artificial intelligence, brain and cognitive sciences, and hardware have fueled a paradigm shift in the role of robotics within society. As robots move from working for people in factory assembly lines to working *with* people—in homes, education, disaster relief, healthcare, manufacturing and more—there is a pressing need to design how humans and robots will interact in these new domains. Much of past research has focused on models for robots operating in isolation. However, as humans and robots become collaborators in a shared space, it is important to consider the development of models that encapsulate the expectations of the human agent. In this review, we explore emerging research in a range of domains **to offer a perspective on building models for action parsing and goal inference in human-robot collaboration.**

Consider the scene of a dinner table. Despite the presence of multiple actors and objects, humans are easily able to reason about how to manipulate the environment in a way that is intuitive to others involved. By using a combination of sensory inputs and human cues–gaze, hand configurations, sounds and hand trajectory to name a few–humans are able to rapidly navigate the situation to accomplish their goals. Now consider a robot participating in this scene. The robot must be able to reason about a shared space with multiple human agents. How should the robot communicate its goals to others? What if the robot takes a path that is unintuitive to an agent, causing a collision? How does the robot know what object the human is reaching for? How should the robot plan its trajectory to its goal? These questions present a compelling context in which to explore the development of human-centric computational models for robot motion planning.

Effective human-robot cooperation requires “the ability to understand a teammate’s actions, as well as to infer their goals and other mental states that are responsible for generating those actions” [[6]](#ref6). Understanding the importance of contextual cues such as hand configuration, gaze, interaction of hands with objects, and facial movements, not only serves to improve a robot’s ability to predict the actions of human agents, but also to express its own intent in an intuitive way. We emphasize that the problem of action parsing and goal inference in robot motion planning is a complex, and multifaceted one. If we consider the simple task of a robot grabbing a specific object (Figure 1), there are a series of processes that must take place. Firstly, the robot must be able to dynamically assess its surroundings to determine not only potential obstacles which may arise, but also the projected goals of other agents in the space (i). Secondly, the robot must plan a set of actions that are intuitive to the human observer to accomplish its desired goal (ii). And finally, it must execute these actions while adjusting dynamically to changes in the environment (iii).

[image]()

## A Multidisciplinary Approach
Foundational research on how the human brain perceives and analyses its environment has led to pathbreaking applications in many fields. Advancements in understanding the mechanics of the human auditory system led to the use of psychoacoustic modelling to dramatically improve audio compression in modern audio codecs. The evolution of convolutional neural networks in AI was inspired by the work of Hubel and Wiesel on receptive fields in cat and monkey visual cortex [[1]](#ref1). Similarly, growing research on how the human mind rapidly predicts the actions and intent of other human agents has led to a growing body of research that has direct applications in human-robot collaboration.

Enabling effective human-robot collaboration in action-goal contexts demands two fundamental aspects: **(i) robots inferring the beliefs and goals of human agents and (ii) robots planning actions that are intuitive and helpful to humans.** Given the dynamic nature of contextual cues in the human environment, it is important to note that these processes must occur concurrently. Deriving insight into how we can move towards such an integrated system demands an exploration of recent research at the intersection of a range of domains. Below we present a set of emerging research in brain and cognitive science, human-robot interaction, and artificial intelligence and its application to this domain.

### Robots interpreting human agents
The problem of interpreting actions and goals of human agents in a shared environment requires mechanisms for identifying contextual cues such as hand configurations, their interaction with objects, gaze, and facial movements [[9]](#ref9). While many state-of-the-art statistical models struggle with modeling these “natural cognitive aspects”, the ability to make inferences and predictions in an environment comes naturally to humans. [[9]](#ref9). Developmental studies show that infants as young as six months are able to infer goals or desires based on people’s “reaching” motion [[10]](#ref10) or “mover events”. In “From simple innate biases to complex visual concepts”, Shimon, Harari and Dorfman propose computational models built using unsupervised learning that are guided by such “mover events”. Here an unsupervised agent learns to detect gaze and hands by using the empirically motivated concept of detecting regions of the image that are moving as an internal teaching signal for learning. The model combines different ideas observed in infant learning–an ability to detect faces, the concept of “mover” events and body context–in conjunction with appearance detection, to learn to detect hands and predict gaze [[9]](#ref9). Thus, a computationally complex task is broken down into a sequence of simpler teaching cues that are used to co-train an unsupervised classifier to achieve similar results as a fully supervised classifier. Using such models inspired by human developmental psychology in the human-robot collaboration context can be foundational in two regards–firstly, for modeling how a robot detects the human agent’s actions through cues, and secondly, in how a robot emits similar cues to signify its goals to human observers.

A natural progression from identifying contextual cues is the problem of interpreting them. When a robot and human collaborate, the robot must be able to use the detected cues to infer the goals and expectations of the human agent. Research shows that infants expect agents to “act in a goal-directed, e cient, and socially sensitive fashion”[[8]](#ref8). However, the fundamental approach to computational models that emulate this rationale for artificially intelligent agents is yet to be agreed upon [[8]](#ref8).

In “Modeling Human Plan Recognition using Bayesian Theory of Mind” [[2]](#ref2), Baker and Tenenbaum propose a Bayesian framework to model the “Theory of Mind”(ToM)–our brain’s intuitive conception of the states of other agents–specifically in the context of belief and desire dependent planning. Interestingly, the paper also explores “reverse engineering” human ToM as a “promising avenue for building plan recognition systems” [[2]]((#ref2). This same idea is built upon in another paper, “Inferring Human Intent from Video by Sampling Hierarchical Plans” [[7]](#ref7). Here, Holtzen et al. discuss how inferring a robot’s goals based on passive observation is critical to integrating robots in our daily lives. This study places the robot in the role of a passive observer focused on determining the intended goal of a human actor based on “limited knowledge and partial observations”–much like the way people perceive goals amongst one another. This work, builds on the ToM framework and “principle of rationality” to infer a human agent’s intent by reverse engineering the action planning process using a Bayesian probabilistic programming framework. The human agent in their experiment has a hierarchical task that is revealed over time to the passive “robot” observer. An And-Or graph over all possible plans, actions and states is constructed and its probabilities are tuned with each subsequent observation by generating optimal trajectories using a basic motion planner, to build a Bayesian probabilistic framework for determining the intent of the human agent[[7]](#ref7).

### Generating intuitive robot behavior
For people and robots to effectively collaborate on tasks, a robot must go beyond interpreting human agents to plan its goals and corresponding actions through a space. However, “most motion in robotics is purely functional: industrial robots move to package parts, vacuuming robots move to suck dust, and personal robots move to clean up a dirty table” [[5]](#ref5). While the problem of robot motion planning in isolation has been studied extensively, there is little research exploring this motion in the presence of a human agent. Building models for robot motion planning that incorporate the expectations of a human collaborator requires a sophisticated understanding of how people reason about action-to-goal behaviors and their expectations of artificial agents.

#### Human as passive observer
In “Integrating Human Observer Inferences into Robot Motion Planning” [[5]](#ref5), Dragan and Srinivasa propose a framework for robot motion planning in the context of a human observer by formalizing two key inferences in opposing directions, legibility (action-to-goal) and predictability (goal-to-action). Using the principle of rational action as a basis, this work uses functional gradient descent to solve for predictable motion as a cost function with the intent to maximize e ciency, where e ciency encompasses “every aspect of the observer’s expectation including body motion, hand motion, arm motion and gaze”[5]. With action interpretation theory as a basis, legibility is modeled utilizing a Bayesian approach where the probability of the particular goal is evaluated conditional on the trajectory from the perspective of the observer. Dragan and Srinivasa offer methodologies for combining these inferences into motion planning models, highlighting the tradeoffs that occur due to the complexity of action interpretation in humans. In “Effects of Robot Motion on Human-Robot Collaboration” [[4]](#ref4) Dragan et al. extend this work, asking human agents to predict the intent of a robot, in the context of collaborating with human agents to fulfill orders in a coffee shop. They use the experiment to measure which motion plans were most intuitive to the human observer, and conclude that human agents had a preference for the more legible motion plan.

#### Towards an integrated approach
While Dragan and Srinivasa’s approach places the human agent as a passive observer in the context of human-robot collaboration, Gray, Breazeal et al. present a framework for integrating the ability to infer human intent with robot behavior generation. In “Action parsing and goal inference using self as simulator”, Gray, Breazeal et al. find inspiration in the biological mechanisms underlying “mirror neurons” and the premise that “certain parts of the brain have dual use; they are used not only to generate our own behavior and mental states, but also to predict and infer the same in others”[[6]](#ref6). Initially, a robot capable of recognizing imitating human movements is trained through a reinforcement learning based “do as I do” imitation game [[6]](#ref6) on a series of human poses. As the robot observes the human conduct a complex task, it uses automatic segmentation techniques to break down the stream into a series of small actions and maps the perceived body positions into its own joint space. This joint angle data is processed through the robot’s pose-graph to identify the motion most similar to that of the human. Parsing this set of “dual-pathway” information and surrounding contextual cues, the robot then generates goal and belief inferences regarding the human and plans actions to provide assistance. [[6]](#ref6) This approach is tested in the context of a robot assisting a human teammate on a task involving three buttons which must be turned ON and OFF with varying degrees of difficulty.

## Converging Ideas
A review of the recent research in human-robot collaboration for task-based behavior exposes a natural clustering of approaches built on similar foundational ideals. While the research papers presented differ in their originally intended application and domain, they present a compelling premise for synthesizing a model for collaboration between humans and robots.

Action parsing and goal inference presents a particularly challenging context where “significant and meaningful features can be non-salient and highly variable and therefore difficult to learn” [[9]](#ref9). Given the ease with which humans reason about such tasks, the notion of leveraging a combination of “learning and innate mechanisms” [[9]](#ref9) presents a compelling foundational idea for machines acquiring similar abilities. Shimon et al. present such an approach by using empirical evidence of how babies learn to recognize hands and gaze direction as the basis for an unsupervised classifier. We see this again in Gray, Breazeal et al. leveraging the mechanisms of “mirror neurons” as a basis for building a simulation-based approach to human-robot interaction [[6]](#ref6).

In our review of emerging work, we also see significant recurrence of Bayesian models built on ToM and different variants of the “principle of rationality”. Both Holtzen et al. and Dragan and Srinivasa leverage elements from Bayesian ToM as assumptions for the work presented and show symmetry in their experimental designs. However, their treatment of robots and human agents in the shared context is entirely different. In Holtzen et al., we had a passive “robot” observer tracking human movements using a state of the art RGBD tracking algorithm in a living room scene and an o ce scene containing about 30 candidate goals for any given frame. The robot was trying to jointly infer object recognition, action detection, and intent. In Dragan and Srinivasa, the experimental setup involved a coffee shop scenario in which a human observer needs to decide which cup the robot is reaching for and then choose the right ingredients to “collaborate” in fulfilling the order[[4]](#ref4). In this case, the human observers are trying to infer the actions and intent of the robot. The robot used three different motion plans–functional, predictable and legible–and the human observers were measured for their ability to recognize the intent in each case.[[4]](#ref4) While both papers explore similar goal-based contexts, the roles of the human and the robot are inverted.

## The Emergence of Two Views
At this stage, we see two different paradigms emerge regarding the design of robots for human-robot collaboration:

    1. Human beings perceive robots differently from themselves
    2. Human beings project “human-like” qualities to robots and expect them to behave like human agents

In the first view, we must seek to develop a “theory of robots” that is intuitive to a human agent. Dragan and Srinivasa’s explore this approach by evaluating the tradeoffs between the competing infer- ences of legibility and predictability in human’s perceptions of a robots path. When asking users to draw expected trajectories of a given robot, people struggled with providing a synthesized view. Thus, “e ciency of robot motion has different meanings for different observers” and a “predictable robot would have to adapt to the particulars of a human observer” [[5]](#ref5). In another paper exploring human-robot collaboration in the context of a coffee shop, Dragan et al. find that legible motion “leads to more fluent collaborations than predictable motion, planned to match the collaborator’s expectations”[[4]](#ref4). While the first case presents an important point around people lacking cohesive expectations of artificial agents, it is important to note that these results were derived in the absence of prior training. Similarly, in the second condition, the human actors were exposed to a very limited number of trials (two normal trials before an ambiguous trial), before measuring their preferred path.

Drawing on studies referenced by Shimon, Harari and Dorfman showing that “su cient exposure to mover events by an artificial mover is predicted to promote hand-like expectations in humans”[[9]](#ref9), it can be argued that given more trials, the human agent’s could alter their expectation of robots over time.

This forms the basis for us to argue that a second paradigm, in which our knowledge of how we predict the intent of human agents can be used to build more “human-like” robots, is more fundamental. Here, we extend the ideas discussed in previous sections even further and argue that an ability to not only passively detect the intent of a human agent, but to also behave like one and emit important cues, is of critical importance in human-robot collaboration. In “Bayesian models of human action understanding”, Baker, Tenenbaum and Saxe argue that “adults and infants have been shown to make robust and rapid intentional inferences about agents’ behavior, even from highly impoverished stimuli”[[3]](#ref3). Human beings constantly believe that most actions of other human agents are rational and infer intent using this stance. This ability to rapidly apply an “intentional stance” seems to be a “core capacity of human cognition”[[3]](#ref3). If human beings were to believe that the robots in a scene are “human-like”, they can project these expectations of rationality and the “intentional stance” on the robot[[3]](#ref3). Thus, a robot which uses cues like rational and efficient motion plans and gaze to communicate intent, would be instantly intuitive in a collaboration setting. Gray, Breazeal et al present a methodology for generating such human-like expectation of robots through a reinforcement learning based “do as I do” imitation game and simulation based approach.

## Challenges Ahead
As robots enter the everyday lives of people, there is a greater need to understand and design how people and robots will interact. The research presented in this review shows a range of approaches to specific sub-problems within human-robot collaboration. However, to integrate robots seamlessly into people’s lives, a synthesis of these ideas is crucial–with much remaining to be understood before a robot is ready for the dining table.

Such a robot must not only use rapid inference from Bayesian models to predict the intent of human agents in its surroundings, but also emulate human behaviour in its own motion. The robot must also be capable of making, and iterating on complex decisions in real-time based on the actions of human agents in the environment. We emphasize that this kind of ideal dynamic interplay between analysing and understanding the environment while simultaneously planning motion presents a complex and challenging problem. Through exploring the problem of action parsing and goal inference from a range of perspectives, we aim to synthesize a perspective on how these views converge and differ. We believe that research at the intersection of these fields will bring us closer to a future where we collaborate productively and intuitively with robots.

## References

<p id="ref1">
    [1] Convolutional neural networks (lenet), 2008.
</p>
<p id="ref2">
    [2] Chris L Baker and Joshua B Tenenbaum. Modeling human plan recognition using bayesian theory of mind. 2013.
</p>
<p id="ref3">
    [3] Chris L. Baker, Joshua B. Tenenbaum, and Rebecca Saxe. Bayesian models of human action under- standing. In NIPS, 2005.
</p>
<p id="ref4">
    [4] Anca D. Dragan, Shira Bauman, Jodi Forlizzi, and Siddhartha S. Srinivasa. Effects of robot motion on human-robot collaboration. In HRI, 2015.
</p>
<p id="ref5">
    [5] Anca D. Dragan and Siddhartha S. Srinivasa. Integrating human observer inferences into robot motion planning. Auton. Robots, 37:351–368, 2014.
</p>
<p id="ref6">
    [6] Jesse Gray, Cynthia Breazeal, Matt Berlin, Andrew G. Brooks, and Jeff Lieberman. Action parsing and goal inference using self as simulator. In RO-MAN, 2005.
</p>
<p id="ref7">
    [7] Steven Holtzen, Yibiao Zhao, Tao Gao, Joshua B Tenenbaum, and Song-Chun Zhu. Inferring human intent from video by sampling hierarchical plans. 2016.
</p>
<p id="ref8">
    [8] Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, and Samuel Gershman. Building machines that learn and think like people. CoRR, abs/1604.00289, 2016.
</p>
<p id="ref9">
    [9] Shimon Ullman, Daniel Harari, and Nimrod Dorfman. From simple innate biases to complex visual concepts. Proceedings of the National Academy of Sciences of the United States of America, 109 44:18215–20, 2012.
</p>
<p id="ref10">
    [10] Amanda Woodward. ” how infants make sense of intentional action ”. 2001.
</p>
